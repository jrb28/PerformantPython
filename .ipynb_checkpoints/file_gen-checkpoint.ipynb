{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Large Data File\n",
    "\n",
    "Create a version of Norfolk weather file that is 1M+ lines long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/NorfolkWeather1999.csv', 'r') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "with open('files/NorfolkWeatherLong.csv', 'w') as f: \n",
    "    f.write('\\n'.join(data for _ in range(3000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer, the cell below creates 1M lines of random temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open('files/NorfolkWeatherLong.csv', 'w') as f:\n",
    "    f.write('\\n'.join([str(i) + ',' + str(100*random.random()) for i in range(1_000_000)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice \n",
    "with open('files/NorfolkWeather1999.csv', 'r') as f:\n",
    "    _ = f.readline()\n",
    "    print([*islice(f,5)])\n",
    "    print([*islice(f,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/NorfolkWeather1999.csv', 'r') as f:\n",
    "    f."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "cs = [1000, 10000, 100000, 1000000, 2000000, 5000000, 10000000]\n",
    "times = {}\n",
    "for chunksize in cs:\n",
    "    start = time.time()\n",
    "\n",
    "    fh = pd.Series(dtype='int64')\n",
    "    for chunk in pd.read_csv('files/new-york-city-taxi-fare-prediction/train.csv', chunksize=2000000):\n",
    "        fh = fh.add(chunk['passenger_count'].value_counts(), fill_value=0)\n",
    "    print(fh)\n",
    "    et = float(time.time() - start)\n",
    "    print(f'Execution time: {et: .2f} seconds')\n",
    "    times[chunksize] = float(time.time() - start)\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "with gzip.open('files/title.crew.tsv.gz', 'rb') as f:\n",
    "    data = f.readlines()\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Number of Guests in Restaurant Parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dist = ((0.1,1), (.4,2), (.15,3), (.15,4), (0.05,5), (0.07,6), (0.01,7), (0.01,8), (0.01,10), (0.01,12), (0.04,99))\n",
    "cum_dist = [sum([x[0] for x in p_dist][:i+1]) for i in range(len(p_dist))]\n",
    "cum_dist[-1] = 1.0\n",
    "num_guest = [x[1] for x in p_dist]\n",
    "cum_dist, num_guest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "num_obs = 10000\n",
    "obs = []\n",
    "for i in range(num_obs):\n",
    "    rv = random.random()\n",
    "    j = 0\n",
    "    while cum_dist[j] < rv:\n",
    "        j += 1\n",
    "    obs.append(num_guest[j])\n",
    "with open('files/guests.json', 'w') as f:\n",
    "    json.dump(obs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/guests.json', 'r') as f:\n",
    "    x = json.load(f)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small File for Bluebikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "url = 'https://jrbrad.people.wm.edu/data/ctba/bluebikes.csv'\n",
    "\n",
    "response = requests.get(url)\n",
    "data = (line.decode('utf-8') for line in response.iter_lines())\n",
    "print(time.time() - start)\n",
    "\n",
    "data = [next(data) for _ in range(103)]\n",
    "with open('bluebikes_small.csv', 'w') as f:\n",
    "    f.write('\\n'.join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/bluebikes_small.csv', 'r') as f:\n",
    "    data = f.readlines()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take out empty lines of Bluebikes data and replace decoding issues\n",
    "\n",
    "__Big takeaway:__ the methods <code>response.iter_lines()</code> is not reliable.  Do not use it to retrieve Internet data.  Instead, use <code>response.text.split('\\r\\n')</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "''' Get the large data set from the Internet with the requests module '''\n",
    "''' This code creates a generator '''\n",
    "url = 'https://jrbrad.people.wm.edu/data/ctba/bluebikes.csv'\n",
    "#with requests.get(url) as f:   # , stream=True\n",
    "#    data = (line.decode('utf-8') for line in f.iter_lines())\n",
    "    \n",
    "response = requests.get(url)\n",
    "data = (line.decode(encoding='utf-8', errors='replace').replace(u'\\uFFFD', '/') for line in response.iter_lines())\n",
    "\n",
    "with open('files/bluebikes_clean.csv', 'w') as f:\n",
    "    f.write('\\n'.join([*data]))\n",
    "    \n",
    "print(f'Acquisition time: {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Monument Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "with open('files/nri-AntActspsht2021-10-21.csv', 'r', encoding='utf-8', errors='replace') as f:\n",
    "    data = f.read()\n",
    "data =unicodedata.normalize('NFD', data).encode('ascii', 'replace').decode('utf-8')\n",
    "with open('files/nri-AntActspsht2021-10-21_.csv', 'w') as f:\n",
    "    for d in data:\n",
    "        try:\n",
    "            f.write(d)\n",
    "        except:\n",
    "            print(d)\n",
    "#data = [d.strip().split(',') for d in data]\n",
    "''' Put your code here '''\n",
    "\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
